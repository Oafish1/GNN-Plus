{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b0ac0f1-9a18-4e03-954a-4d272c3b5d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pecanpy as pp\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import auc, confusion_matrix, precision_recall_curve, roc_curve\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fca094d-6ce4-4a96-8fdd-e2bf01c5c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0143b74-a2c8-493b-83a6-8a684b8d8ab1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46e4613c-a6f0-4596-bd91-04a635309a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = {}\n",
    "def get_data(cell_type):\n",
    "    # Get graph\n",
    "    data_folder = './data/training_matrices_DGL/'\n",
    "    if cell_type in loaded_data:\n",
    "        mat = loaded_data[cell_type]\n",
    "    else:\n",
    "        mat = pd.read_csv(data_folder + f'{cell_type}.feat.mat', index_col=0, sep='\\t')\n",
    "        loaded_data[cell_type] = mat\n",
    "    genes = np.array(mat.columns)\n",
    "    assert (genes == np.array(mat.index)).all()\n",
    "\n",
    "    # Get labels\n",
    "    meta = pd.read_csv(data_folder + 'training_labels.txt', index_col=0, sep='\\t')\n",
    "    labels = meta.values[:, 0]\n",
    "    labels = np.array([str(meta.loc[g]['label']) if g in meta.index else 'unknown' for g in genes])\n",
    "\n",
    "    return mat.to_numpy(), labels, genes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d4a867-2ea0-4c07-aca7-060353ddc80a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8763874-6d70-4148-a1d2-c3d1e140c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type = 'Astrocyte'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d407569f-ecff-4a5f-b328-e0d0fa4dab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Astrocyte data...\n"
     ]
    }
   ],
   "source": [
    "print(f'Loading {cell_type} data...')\n",
    "# Load data\n",
    "data, labels, genes = get_data(cell_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85d25e6-1501-4fa3-817f-8aa13de2d3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing graph...\n"
     ]
    }
   ],
   "source": [
    "print('Constructing graph...')\n",
    "# Construct graph\n",
    "g = pp.graph.AdjlstGraph()\n",
    "for i in range(data.shape[0]):\n",
    "    for j in range(i+1, data.shape[0]):\n",
    "        weight = float(data[i][j])\n",
    "        if weight != 0:\n",
    "            g.add_edge(genes[i], genes[j], weight=weight, directed=False)\n",
    "g.save('_elist.edg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a40a4027-6f59-405c-baa5-e0541df39b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Node2Vec+...\n",
      "\tReading graph...\n",
      "\tGenerate embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f05183a65e74c43a6fba278b04402c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                        | 0/108020 [00:00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Running Node2Vec+...')\n",
    "print('\\tReading graph...')\n",
    "# Load as precomp\n",
    "g = pp.pecanpy.SparseOTF(p=1, q=1, workers=4, verbose=True)\n",
    "g.read_edg('_elist.edg', weighted=True, directed=False)\n",
    "# g.preprocess_transition_probs()\n",
    "\n",
    "print('\\tGenerate embeddings...')\n",
    "# Generate embeddings\n",
    "dim, num_walks, walk_length = 16, 10, 20\n",
    "emb = g.embed(dim=dim, num_walks=num_walks, walk_length=walk_length)\n",
    "np.save(f'embeddings-{emb.shape[1]}-{num_walks}-{walk_length}.npy', emb)\n",
    "\n",
    "# Chart connected subgraph\n",
    "surviving_nodes = [np.argwhere(genes==gn).flatten()[0] for gn in g.nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "897d0d58-21db-46eb-8303-a1a60ddfe2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...\n"
     ]
    }
   ],
   "source": [
    "print('Splitting data...')\n",
    "# Generate train_idx\n",
    "train_frac = .8\n",
    "train_idx = np.random.choice(emb.shape[0], int(train_frac * emb.shape[0]), replace=False)\n",
    "train_idx = np.intersect1d(train_idx, np.array(list( set(list(range(emb.shape[0]))) - set(list(np.argwhere(labels[surviving_nodes]=='unknown').flatten())) )))\n",
    "eval_idx = np.array(list(set(list(range(emb.shape[0]))) - set(train_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b694892-1a36-4866-882d-5aa4038b63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCL(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=None, output_dim=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        hidden_dim = hidden_dim if hidden_dim is not None else int(input_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            # nn.Dropout(.8),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            # nn.Dropout(.8),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.BatchNorm1d(output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.main(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f584761b-5790-4ba3-b26c-7bc2a0e1f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating classification model...\n",
      "Epoch 0:\tTrain_Loss 0.815,\tEval_Loss 0.799\n",
      "Epoch 25:\tTrain_Loss 0.695,\tEval_Loss 0.706\n",
      "Epoch 50:\tTrain_Loss 0.631,\tEval_Loss 0.680\n",
      "Epoch 75:\tTrain_Loss 0.591,\tEval_Loss 0.676\n",
      "Epoch 100:\tTrain_Loss 0.564,\tEval_Loss 0.628\n",
      "Epoch 125:\tTrain_Loss 0.544,\tEval_Loss 0.599\n",
      "Epoch 150:\tTrain_Loss 0.537,\tEval_Loss 0.597\n",
      "Epoch 175:\tTrain_Loss 0.523,\tEval_Loss 0.588\n",
      "Epoch 200:\tTrain_Loss 0.518,\tEval_Loss 0.574\n",
      "Epoch 225:\tTrain_Loss 0.500,\tEval_Loss 0.564\n",
      "Epoch 250:\tTrain_Loss 0.496,\tEval_Loss 0.551\n",
      "Epoch 275:\tTrain_Loss 0.487,\tEval_Loss 0.553\n",
      "Epoch 300:\tTrain_Loss 0.479,\tEval_Loss 0.563\n",
      "Epoch 325:\tTrain_Loss 0.468,\tEval_Loss 0.554\n",
      "Epoch 350:\tTrain_Loss 0.465,\tEval_Loss 0.538\n",
      "Epoch 375:\tTrain_Loss 0.451,\tEval_Loss 0.527\n",
      "Epoch 400:\tTrain_Loss 0.448,\tEval_Loss 0.514\n",
      "Epoch 425:\tTrain_Loss 0.435,\tEval_Loss 0.510\n",
      "Epoch 450:\tTrain_Loss 0.432,\tEval_Loss 0.518\n",
      "Epoch 475:\tTrain_Loss 0.425,\tEval_Loss 0.517\n",
      "Epoch 493\tTrain_Loss 0.421,\tEval_Loss 0.531\n",
      "Stopped!\n"
     ]
    }
   ],
   "source": [
    "print('Creating classification model...')\n",
    "# Balanced train\n",
    "batch_size = 128\n",
    "max_lapses = 100\n",
    "\n",
    "mlp = FCL(emb.shape[1])\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-5)\n",
    "criterion = nn.BCELoss()\n",
    "best_loss = torch.inf; lapses = 0\n",
    "for epoch in range(10001):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    mlp.train()\n",
    "    for batch in range(emb.shape[0] // batch_size):\n",
    "        seg_size = int(batch_size / 2)\n",
    "        ad_batch_idx = np.random.choice(\n",
    "            np.intersect1d(train_idx, np.argwhere(labels[surviving_nodes]=='AD').squeeze()),\n",
    "            seg_size,\n",
    "            replace=True)\n",
    "        notad_batch_idx = np.random.choice(\n",
    "            np.intersect1d(train_idx, np.argwhere(labels[surviving_nodes]=='notAD').squeeze()),\n",
    "            seg_size,\n",
    "            replace=True)\n",
    "        batch_idx = np.concatenate([ad_batch_idx, notad_batch_idx])\n",
    "\n",
    "        true = torch.cat([torch.ones((seg_size, 1)), torch.zeros((seg_size, 1))])\n",
    "        logits = mlp(torch.Tensor(emb[batch_idx]))\n",
    "        loss = criterion(logits, true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach()\n",
    "    epoch_loss = epoch_loss / (emb.shape[0] // batch_size)\n",
    "    \n",
    "    # Evaluation loss\n",
    "    mlp.eval()\n",
    "    logits = mlp(torch.Tensor(emb[eval_idx])).detach().squeeze()\n",
    "    trans = {'AD': 1, 'notAD': 0, 'unknown': 2}\n",
    "    trans_inv = {v: k for k, v in trans.items()}\n",
    "    true = torch.Tensor([trans[l] for l in labels[surviving_nodes][eval_idx]])\n",
    "    eval_loss = criterion(logits[np.argwhere(true!=2).squeeze()], true[np.argwhere(true!=2).squeeze()]).detach()\n",
    "    \n",
    "    # CLI\n",
    "    if epoch % 25 == 0:\n",
    "        print(f'Epoch {epoch}:\\tTrain_Loss {float(epoch_loss):.3f},\\tEval_Loss {float(eval_loss):.3f}')\n",
    "    \n",
    "    # Early stopping\n",
    "    lapses += 1\n",
    "    if eval_loss < best_loss:\n",
    "        best_loss = eval_loss\n",
    "        lapses = 0\n",
    "    if lapses >= max_lapses:\n",
    "        print(f'Epoch {epoch}\\tTrain_Loss {float(epoch_loss):.3f},\\tEval_Loss {float(eval_loss):.3f}')\n",
    "        print('Stopped!')\n",
    "        break\n",
    "mlp.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b73f76f-97c4-4253-b6fe-340192c8adba",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb3ec0a0-62e6-4aef-9211-5e0a99bbf447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating performance...\n",
      "Train\n",
      "T\\P\tnotAD\tAD\n",
      "notAD\t1943\t469\n",
      "AD\t1\t102\n",
      "AUROC:\t0.9478\n",
      "AUPRC:\t0.3194\n",
      "\n",
      "Eval\n",
      "T\\P\tnotAD\tAD\tunknown\n",
      "notAD\t437\t157\t0\n",
      "AD\t14\t4\t0\n",
      "unknown\t5905\t1770\t0\n",
      "AUROC:\t0.4988\n",
      "AUPRC:\t0.0293\n",
      "\n",
      "Recording predictions...\n"
     ]
    }
   ],
   "source": [
    "print('Evaluating performance...')\n",
    "print('Train')\n",
    "## Train\n",
    "# Perform prediction\n",
    "logits = mlp(torch.Tensor(emb[train_idx])).detach().squeeze()\n",
    "trans = {'AD': 1, 'notAD': 0, 'unknown': 2}\n",
    "trans_inv = {v: k for k, v in trans.items()}\n",
    "true = torch.Tensor([trans[l] for l in labels[surviving_nodes][train_idx]]).long()\n",
    "\n",
    "# Get confusion\n",
    "conf = confusion_matrix(true, 1*(logits > .5))\n",
    "print('T\\P\\t' + '\\t'.join([trans_inv[i] for i in range(len(trans_inv)-1)]))\n",
    "for i, row in enumerate(conf):\n",
    "    print(trans_inv[i] + '\\t' + '\\t'.join([str(e) for e in row]))\n",
    "\n",
    "# Other statistics\n",
    "fpr, tpr, thresholds = roc_curve(true[np.argwhere(true!=2).squeeze()], logits[np.argwhere(true!=2).squeeze()])\n",
    "print(f'AUROC:\\t{auc(fpr, tpr):.4f}')\n",
    "prec, rec, thresholds = precision_recall_curve(true[np.argwhere(true!=2).squeeze()], logits[np.argwhere(true!=2).squeeze()])\n",
    "print(f'AUPRC:\\t{auc(rec, prec):.4f}')\n",
    "print()\n",
    "\n",
    "print('Eval')\n",
    "## Eval\n",
    "# Perform prediction\n",
    "logits = mlp(torch.Tensor(emb[eval_idx])).detach().squeeze()\n",
    "trans = {'AD': 1, 'notAD': 0, 'unknown': 2}\n",
    "trans_inv = {v: k for k, v in trans.items()}\n",
    "true = torch.Tensor([trans[l] for l in labels[surviving_nodes][eval_idx]]).long()\n",
    "\n",
    "# Get confusion\n",
    "conf = confusion_matrix(true, 1*(logits > .5))\n",
    "print('T\\P\\t' + '\\t'.join([trans_inv[i] for i in range(len(trans_inv))]))\n",
    "for i, row in enumerate(conf):\n",
    "    print(trans_inv[i] + '\\t' + '\\t'.join([str(e) for e in row]))\n",
    "\n",
    "# Other statistics\n",
    "fpr, tpr, thresholds = roc_curve(true[np.argwhere(true!=2).squeeze()], logits[np.argwhere(true!=2).squeeze()])\n",
    "print(f'AUROC:\\t{auc(fpr, tpr):.4f}')\n",
    "prec, rec, thresholds = precision_recall_curve(true[np.argwhere(true!=2).squeeze()], logits[np.argwhere(true!=2).squeeze()])\n",
    "print(f'AUPRC:\\t{auc(rec, prec):.4f}')\n",
    "print()\n",
    "    \n",
    "print('Recording predictions...')\n",
    "# Write predicted AD to file (only unknown genes)\n",
    "unk_idx = np.argwhere(labels[surviving_nodes][eval_idx]=='unknown').squeeze()\n",
    "np.savetxt('AD.txt', genes[surviving_nodes][eval_idx][np.intersect1d(unk_idx, np.argwhere(logits > .8).squeeze())], fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8433bcb-fc8e-46bc-bf28-0f529a417724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
